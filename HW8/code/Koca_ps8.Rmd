---
title: "Problem Set 8"
author: "Ilayda Koca"
date: "2022-11-11"
output: html_document
---

## Getting Set Up

If you haven't already, create a folder for this course, and then a subfolder within for the second lecture `Topic9_Clustering`, and two additional subfolders within `code` and `data`.

Open `RStudio` and create a new RMarkDown file (`.Rmd`) by going to `File -> New File -> R Markdown...`.
Change the title to `"DS1000: Problem Set 8"` and the author to your full name. Save this file as `[LAST NAME]_ps8.Rmd` to your `code` folder.

If you haven't already, download the `FederalistPaperCorpusTidy.Rds` file from the course [github page](https://github.com/jbisbee1/DS1000-F2022/blob/master/Lectures/Topic9_Clustering/data/CountyVote2004_2020.Rds) and save it to your `data` folder.

All of the following questions should be answered using 

Require `tidyverse`, `tidytext`, and load the `FederalistPaperCorpusTidy.Rds` data to `corpus.tidy`.
```{r}
require(tidyverse)
require(tidytext)
corpus.tidy <-  readRDS('../data/FederalistPaperCorpusTidy.Rds')
```


## Question 1 [4 points]
Calculate the degree to which Madison or Hamilton use a given word by calculating the ratio of Hamilton's use to Madison's use. To do this, start by converting the data into a "bag of words" (BOW) structure using the `unnest_tokens()` function from the `tidytext` package. Make sure to remove any numbers! Then calculate the frequency that either Hamilton or Madison used each word, and finally calculate the ratio of Hamilton to Madison. Now remove any words that appear fewer than 20 times, and then plot the top-10 most Hamilton-specific words and the top-10 most Madison-specific words. Do you see any interesting patterns?

```{r}
tokens <- corpus.tidy %>%
  unnest_tokens(output = word, input = text,
                token = "word_stems") %>%
  mutate(word = str_replace_all(word, "\\d+", "")) %>%
  filter(word != "")

author_ratio <- tokens %>%
  count(author, word) %>%
  filter(author %in% c('hamilton', 'madison')) %>%
  spread(author, n) %>%
  rowwise() %>%
  mutate(ratio = hamilton / madison, total = sum(hamilton, madison, na.rm=T))

madison_words <- author_ratio %>%
  filter(total > 20 & (ratio > 5 | ratio < 1) & ratio != "Inf") %>%
  arrange(ratio)

hamilton_words <- author_ratio %>%
  filter(total > 20 & (ratio > 5 | ratio < 1) & ratio != "Inf") %>%
  arrange(ratio)

madison_words %>%
  ungroup() %>%
  arrange(ratio) %>%
  slice(1:10) %>%
  ggplot(aes(x = ratio, y = reorder(word, ratio))) +
  geom_bar(stat = "identity") +
  labs(x = 'Ratio',
       y = 'Word',
       title = 'Top 10 Madison-Specific Words')

madison_words

hamilton_words %>%
  ungroup() %>%
  arrange(-ratio) %>%
  slice(1:10) %>%
  ggplot(aes(x = ratio, y = reorder(word, ratio))) +
  geom_bar(stat = "identity") +
  labs(x = 'Ratio',
       y = 'Word',
       title = 'Top 10 Hamilton-Specific Words')

hamilton_words
         
```

> - Madison's use of words of confeder, congress, inhabit, depart indicate that Madison might be talking about the Civil War. Hamilton uses th words contribut, station, communiti indicates that he is focusing on issues regarding America as a nation.

## Question 2 [4 points]
Now **wrangle** the data in order to run a regression in which you predict either Hamilton or Madison authorship as a function of the rate at which the top-5 most specific words for each author are used in each document. To do this, you first must create a document term matrix (DTM) and calculate the rate at which words are used (calculate the rate per 1,000 words for this step). Then you must spread the data so that you have a dataset you can use for regression analysis, in which each row is a document, and each column is a word, and the values are the rate at which that word is used in that document. Be careful to change the name of the `author` column to avoid replacing it with the rate at which the word `author` appears in the data! Also make sure to replace missing data (`NA`) with zeros! Finally, recode author so that the outcome is numeric, and is +1 if the author is Hamilton, and is -1 if the author is Madison, and is `NA` otherwise.

```{r}
dtm <- tokens %>%
  count(author,document,word) %>% # create a document-term matrix
  group_by(document) %>%
  mutate(totWords = sum(n)) %>% # Calculate the total words
  ungroup() %>%
  mutate(rate = n*1000 / totWords) # Calculate the rate at which a word is used

dat <- dtm %>%
  select(-n) %>%
  rename(authorFOUNDER = author) %>% 
  spread(word,rate) # Spread

dat <- dat %>%
  mutate_at(vars(-authorFOUNDER,-document,-totWords),
            function(wrd) ifelse(is.na(wrd),0,wrd))
dat
```

## Question 3 [4 points]
Finally, run the regression and use the model to predict authorship on the full data. Visualize the results by plotting the list of Federalist papers on the x-axis and the predicted authorship on the y-axis, coloring points by whether they were authored by Madison, Hamilton, or are contested papers. According to this analysis, who most likely authored the contested documents? EXTRA CREDIT: calculate the 100-fold cross validated RMSE with an 80-20 split, and then express your predictions about authorship in terms of lower and upper bounds. `set.seed(123)` for consistency.

```{r}
datSubset_ham5 <- dat %>%
  select(author = authorFOUNDER, # select() can rename variables with the = 
         document,
         hamilton_words$word[1:5]) # Only grabbing the top 10 words favored by Hamilton
datSubset_ham5

train <- datSubset_ham5 %>%
  filter(author == 'hamilton'|author == 'madison') %>% # subsetting to documents written by either of the two authors we're interested in
  mutate(score = ifelse(author == 'hamilton',1,-1)) # Creating a continuous outcome that is +1 for Hamilton and -1 for Madison
train

form <- paste0('score ~ ',paste(hamilton_words$word[1:5],collapse = ' + '))
form

mHam <- lm(as.formula(form),train)
summary(mHam)

toplotHam <- datSubset_ham5 %>%
  mutate(predAuthor = predict(mHam,newdata = datSubset_ham5)) # Predicting the authorship for every Federalist Paper (including the contested!)
toplotHam %>%
  filter(author %in% c('contested','madison','hamilton')) %>% # Plotting only the papers written by Hamilton, Madison, or contested
  ggplot(aes(x = document,y = predAuthor,color = author)) +  # Color points by author
  geom_point() + 
  geom_hline(yintercept = 0,linetype = 'dashed') + 
  labs(title = 'Predicted Authorship of Federalist Papers',
       x = 'Federalist Paper Number',
       y = 'Predicted Author\n(<0 = Madison, >0 = Hamilton)')
```

> - Positive values on the y-axis mean that our model predicts the author to be Hamilton, while negative values mean that our model predicts Madison. Thus, since more contested documents lie above the dashed line, I think the consented documents were written by Hamilton.

## Question 4 [4 points]
Now open the Trump tweet dataset `Trump_tweet_words.Rds` and save it to an object named `tweet_words`. Also load the sentiment dictionary `nrc` from the `tidytext` package, and look at the words with sentiment scores by merging the two datasets with the `inner_join()` function, which you should save to a new object called `tweet_sentiment`. Using this data, investigate the following research question: do Trump's positive or negative tweets get more engagement (measured with retweets)? To answer this, you will need to first determine whether a tweet is positive or negative by choosing the sentiment that has more words in a tweet. In other words, if a given tweet has two positive words and three negative words, the tweet should be classified as negative. If the tweet has equal number of positive and negative words (or has none), classify it as neutral. Then `group_by()` the sentiment label and add up all the retweets by sentiment. Plot the result and discuss your findings based on visual analysis. Then redo the analysis but take the average retweets by sentiment. Does your conclusion change? If so, why?

```{r}
require(textdata)
tweet_words <- readRDS('../data/Trump_tweet_words.Rds')

nrc <- get_sentiments("nrc")

tweet_sentiment <- tweet_words %>%
  inner_join(nrc, by="word")

tweet_sentiment1 <- tweet_sentiment %>%
  count(document, word, sentiment) %>%
  group_by(document) %>%
  filter(sentiment == "positive" | sentiment == "negative") %>%
  pivot_wider(names_from = sentiment, 
              values_from = n, 
              values_fill = 0) %>%
  mutate(difference = positive - negative) %>%
  mutate(difference = ifelse(difference == 0, 'neutral', ifelse(difference > 0, 'negative', 'positive'))) %>%
  mutate(sentiment = positive - negative)

```

> - Write 4-5 sentences here.

## Question 5 [4 points]
Re-run the previous analysis, except look the results year by year. To do this, you will need to recalculate the average retweets by sentiment for each year. Plot your results over time, with the year in the x-axis and the average retweets on the y-axis, with points colored by sentiment and sized by the total number of tweets falling into each sentiment category per year. Describe your results. What might explain the patterns you observe?

```{r}
tweet_sentiment2 <- tweet_sentiment %>%
  group_by(Tweeting.year) %>%
  summarise(avg_retweets = mean(retweets), na.rm = T) %>%
  ggplot(aes(x = Tweeting.year, y = avg_retweets)) +
  geom_bar(stat="identity") +
  labs(x = "Year", 
       y = "Average Retweets",
       title = "Relationship between average sentiment of Trump's retweets and number of retweets received")

tweet_sentiment2
```

> - Write 2-3 sentences here.

## Question 6 [4 Extra credit points]
Re-answer the research question proposed in Question 4 above comparing Trump pre-presidency to Trump post-presidency. First, state your theory and assumptions. Second, use this theory to generate a hypothesis. Third, evaluate using a linear regression model (`lm()`) in which you predict average retweets as a function of sentiment, subsetting the data first to prior to 2016 and then again, subsetting to 2016 and later. What do you find? **HINT:** You can either run the regression with a categorical version of the $X$ variable, or you can re-calculate sentiment as the difference between the number of positive and negative words in each tweet. **HINT 2:** You should log the retweets (run a univariate visualization if you don't believe me).

```{r}
# INSERT CODE HERE
```

> - Write 2-3 sentences here.